
\documentclass[11pt,a4paper]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{caption}

\lstset{
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{teal},
  breaklines=true,
  frame=single,
  numbers=left,
  numberstyle=\tiny,
  showstringspaces=false
}

\title{Scribe Notes: OpenMP Locks and Mutual Exclusion}
\author{COL380 / OpenMP Lectures}
\date{\today}

\begin{document}
\maketitle

\section{Context and Motivation}
In this lecture segment, we study another important synchronization mechanism in shared-memory parallel programming: \emph{locks}. Locks are used to ensure \textbf{orderly access} to a region of code that must not be executed concurrently by multiple threads. The fundamental property achieved using locks is \textbf{mutual exclusion}.

The discussion is motivated by the need to selectively \emph{sequentialize} parts of an otherwise parallel program. While threads are typically free to execute independently and concurrently, certain code regions---called \emph{critical regions}---must be accessed in a controlled fashion to avoid incorrect behavior.

\section{What Is a Lock?}
A lock is an abstraction that protects a specific region of code. Conceptually:
\begin{itemize}
  \item Multiple threads may want to execute the same region of code $C$.
  \item At most one thread is allowed to execute $C$ at any given time.
  \item Other threads must wait until the currently executing thread exits $C$.
\end{itemize}

In simple terms, a lock \emph{locks} a region of code so that only one thread can be inside that region at a time.

\subsection{Comparison with \texttt{\#pragma omp single}}
At first glance, locks may appear similar to the OpenMP construct:
\begin{lstlisting}
#pragma omp single
{
   C;
}
\end{lstlisting}

However, the semantics are fundamentally different:
\begin{itemize}
  \item \texttt{single}: The code region $C$ is executed by exactly one thread, and never by the others.
  \item \textbf{lock}: Every thread \emph{may eventually execute} $C$, but execution is serialized---one thread at a time.
\end{itemize}

Thus, a lock enables \emph{multiple executions in an orderly fashion}, whereas \texttt{single} enforces a \emph{single execution}.

Importantly, \texttt{single} introduces an implicit barrier at the end (unless \texttt{nowait} is specified), whereas \textbf{locks do not introduce any implicit barrier}. Threads proceed independently after unlocking.

\section{Conceptual Model of Locking}
Consider a shared lock variable $V$:
\begin{itemize}
  \item $V$ resides in shared memory.
  \item Initially, $V = 0$ (unlocked).
  \item A thread that sets $V = 1$ acquires the lock.
  \item Releasing the lock resets $V$ to $0$.
\end{itemize}

All threads compete to acquire $V$. The first thread that succeeds enters the critical region $C$. Other threads wait.

\subsection{Spin Waiting (Na\"ive Lock Implementation)}
In a na\"ive implementation, waiting threads repeatedly check the value of $V$ in a loop:
\begin{lstlisting}[language=C]
while (V != 0) {
    // spin
}
V = 1;  // acquire lock
\end{lstlisting}

This is called \emph{spin waiting} or \emph{busy waiting}. Threads continuously poll the shared variable until the lock becomes available.

Modern processors and runtimes optimize this behavior, but the fundamental idea remains: waiting threads repeatedly check the lock state.

\section{Sequentialization in a Parallel Program}
A natural question arises:

\begin{quote}
If $n$ threads execute the same region $C$ sequentially using a lock, is this equivalent to executing $n$ copies of $C$ sequentially?
\end{quote}

Yes---\textbf{and that is the point}. Locks provide a programming-language-level abstraction that allows the programmer to intentionally serialize execution of a critical region within a parallel program.

Without such an abstraction, threads would execute $C$ concurrently and cause incorrect behavior. Locks give the programmer explicit control over where and how sequentialization occurs.

\section{Fairness, Starvation, and Scheduling}
It is possible for a thread to:
\begin{itemize}
  \item Release a lock,
  \item Reach the next iteration,
  \item And re-acquire the lock again before other threads.
\end{itemize}

This can lead to \textbf{starvation}, where some threads never get a chance to execute $C$. Whether this happens depends on:
\begin{itemize}
  \item The lock implementation,
  \item Scheduling policies,
  \item Hardware and runtime behavior.
\end{itemize}

Designing \emph{fair} synchronization primitives is an active research topic spanning operating systems, architecture, and programming languages.

\section{Race Conditions vs Data Races}
The lecture emphasizes an important distinction:

\begin{description}
  \item[Race condition:] Multiple threads competing to acquire a resource. This is \emph{intentional} and necessary for locks.
  \item[Data race:] Concurrent access to shared data where at least one access is a write and accesses are unordered. This is \emph{incorrect}.
\end{description}

Locks \textbf{allow race conditions} (competition to acquire the lock) but \textbf{prevent data races} inside the protected region.

Since only one thread executes $C$ at a time, shared variables inside $C$ cannot be concurrently modified.

\section{How Do Threads Observe Unlocking?}
Threads waiting for a lock repeatedly read the shared lock variable from memory. Conceptually:
\begin{itemize}
  \item The lock variable $V$ is stored at a memory address.
  \item Waiting threads issue repeated read requests for $V$.
  \item When $V$ changes (unlock), threads observe the update and compete to acquire it.
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics[width=0.85\linewidth]{9a1fc4d5-6aec-4193-be3f-2a694cf8d369.png}
\caption{Slide reference: OpenMP lock primitives and semantics}
\end{figure}

The runtime system ensures that only one thread succeeds in acquiring the lock, even if multiple threads attempt simultaneously.

\section{OpenMP Lock Primitives}
OpenMP exposes lock functionality explicitly through library calls.

\subsection{Lock Types}
\begin{itemize}
  \item \texttt{omp\_lock\_t}: Normal (non-nestable) lock.
  \item \texttt{omp\_nest\_lock\_t}: Nestable lock that allows re-acquisition by the same thread.
\end{itemize}

\subsection{Lock API}
\begin{lstlisting}[language=C]
void omp_init_lock(omp_lock_t *lock);
void omp_set_lock(omp_lock_t *lock);    // acquire
void omp_unset_lock(omp_lock_t *lock);  // release
void omp_destroy_lock(omp_lock_t *lock);

int omp_test_lock(omp_lock_t *lock);    // try-acquire
\end{lstlisting}

\texttt{omp\_test\_lock} returns immediately:
\begin{itemize}
  \item Non-zero if the lock was acquired,
  \item Zero otherwise (no blocking).
\end{itemize}

Typical usage:
\begin{lstlisting}[language=C]
while (!flag) {
    flag = omp_test_lock(&lock);
}
\end{lstlisting}

\section{Key Takeaway}
Locks help achieve \textbf{race-freedom} (specifically, freedom from data races) when accessing a critical region. They enforce \textbf{mutual exclusion} among competing threads without introducing implicit synchronization barriers.

\section{Exercises}
\begin{enumerate}[label=\textbf{Exercise \arabic*:}]
  \item Explain why \texttt{omp\_single} cannot replace locks when all threads must eventually execute a critical region.
  \item Write a small OpenMP program where threads increment a shared counter:
  \begin{itemize}
    \item First without locks (observe incorrect result),
    \item Then using \texttt{omp\_lock\_t}.
  \end{itemize}
  \item Consider a lock that is unfair. Explain how starvation may arise with a looped critical region.
  \item Classify the following as race condition, data race, both, or neither, and justify your answer.
\end{enumerate}

\section{Summary}
This lecture segment introduced locks as a foundational synchronization primitive. Locks provide a controlled way to serialize execution in parallel programs, prevent data races, and expose low-level synchronization control to the programmer. They differ fundamentally from OpenMP directives such as \texttt{single} and are essential for writing correct shared-memory parallel code.

\end{document}
